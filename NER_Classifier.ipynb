{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roger Mei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize,RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_13 = './2013'\n",
    "path_14 = './2014'\n",
    "path_train = './training/'\n",
    "\n",
    "# load training set\n",
    "text_2013 = []\n",
    "text_2014 = []\n",
    "\n",
    "# reading 2013 texts\n",
    "for file in os.listdir(path_13):\n",
    "    text_2013.append(open(os.path.join(path_13,file),'rb').read().decode(\"utf-8\",errors=\"replace\"))\n",
    "    \n",
    "# reading 2014 texts\n",
    "for file in os.listdir(path_14):\n",
    "    text_2014.append(open(os.path.join(path_14,file),'rb').read().decode(\"utf-8\",errors=\"replace\"))\n",
    "    \n",
    "# Combining both 2013 and 2014 texts into one list\n",
    "all_text = text_2013 + text_2014\n",
    "    \n",
    "# reading training data\n",
    "ceo = pd.read_csv(path_train + 'ceo.csv',header=None, encoding='latin-1')\n",
    "companies = pd.read_csv(path_train + 'companies.csv',header=None, encoding='latin-1')\n",
    "percentage = pd.read_csv(path_train + 'percentage.csv',header=None, encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all sentence tokenized articles\n",
    "sentences = []\n",
    "for txt in all_text:\n",
    "    sent = sent_tokenize(txt)\n",
    "    sentences.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Percentage regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one percent/1.0 percent/one.0 percent/1,000,000.00 percent/ (-5,000,000/2) percentage/ 1 - 2 percentage(ile)\n",
    "percentage_pat1 = r'((?:\\(?-?\\w*[,\\d\\/]*(\\.\\d+)? (to |- ))?\\(?-?\\w*[,\\d\\/]*(\\.\\d+)? ?percent(?:age)?(?:ile)?(?: points?)?\\)?)'\n",
    "# one %/1.0 %/-5,000,000.0%/(50.0%)/(5,000,000%)/ one to two %/(-1/2 to 2/232,23013.2 %)/ 1% to 2%\n",
    "percentage_pat2 = r'((?:\\(?-?\\w*[,\\d\\/%]*(\\.\\d+)?%? (to |- ))?\\(?-?\\w*[,\\d\\/]*(\\.\\d+)? ?%\\)?)'\n",
    "\n",
    "#captures pat1 and pat2 without spaces\n",
    "#0.00-0.25%/5,000,000-6,000,000%/6-1/2 %\n",
    "percentage_pat3 = r'\\w+[,\\w\\/\\.]*-\\w+[,\\w\\/\\.]* ?%'\n",
    "# forty-one percent/single-digit percent/50-80 percent/0.00-0.25 percent[age][ile]\n",
    "percentage_pat4 = r'\\w+[,\\w\\/\\.]*-\\w+[,\\w\\/\\.]* ?percent(?:age)?(?:ile)?(?: points?)?\\)?)'\n",
    "\n",
    "# point five percent[age][ile]\n",
    "percentage_pat5 = r'point \\w+ percent(?:age)?(?:ile)?'\n",
    "# half of a percentage point/three-quarters of a percentage point\n",
    "percentage_pat6 = r'\\w+[\\.\\/\\w,]* of a percent(?:age)?(?:ile)? ?(?:point)?'\n",
    "\n",
    "def get_percent(txt):\n",
    "    '''\n",
    "    input:\n",
    "    txt - string: piece of text \n",
    "    return:\n",
    "    p - list: list of matches\n",
    "    Given a piece of text, finds all instances of numbers involving percentages\n",
    "    '''\n",
    "    p = re.findall(percentage_pat1,txt)\n",
    "    p = [group[0] for group in p]\n",
    "    \n",
    "    p2 = re.findall(percentage_pat2,txt)\n",
    "    # only take the full group\n",
    "    p2 = [group[0] for group in p2]\n",
    "    p += p2\n",
    "    \n",
    "    p3 = re.findall(percentage_pat3,txt)\n",
    "    p += p3\n",
    "    \n",
    "    p4 = re.findall(percentage_pat4,txt)\n",
    "    p += p4\n",
    "    \n",
    "    p5 = re.findall(percentage_pat5,txt)\n",
    "    p += p5\n",
    "    \n",
    "    p6 = re.findall(percentage_pat6,txt)\n",
    "    p += p6\n",
    "    \n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = []\n",
    "for article in sentences:\n",
    "    for sentence in article:\n",
    "        matches = get_percent(sentence)\n",
    "        # if there are matches, then append (sentence, match) to list of examples\n",
    "        if len(matches):\n",
    "            for m in matches:\n",
    "                result = (sentence,m)\n",
    "                examples.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sober Look As the percentage of money-losing firms rises, corporate default rates should follow.',\n",
       " 'the percentage')"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[407] # \"the\" (a stop word) begins a negative example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('This chart (via John Brynjolfsson) puts it all in context, showing the 16-week percent change in 10-Year yields over the last few years.',\n",
       " '16-week percent')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[425] # \"the\" precedes a negative example as in \"the 16-week percent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The unemployment rate for July to September 2013 was 7.6% of the economically active population, down 0.2 percentage points from April to June 2013.',\n",
       " '0.2 percentage points')"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[5000] # down/up precedes a positive example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Bernanke ended up doing the opposite – he essentially encouraged the rise in yields, saying it was for good reasons – and investors began dumping bonds, sending the yield on the 10-year Treasury note to 2.35% at the close, up 14 basis points from where it was before the release of the FOMC monetary policy statement and subsequent Bernanke presser this afternoon.',\n",
       " 'note to 2.35%')"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[6] # a noun, not a cardinal number, begins a negative example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all matches that are two words in length, throw away any matches that begin with a stop word. Also remove any matches that begin with an adjective ('JJ') such as same, fixed, small, and big. Matches usually begin with a cardinal number ('CD'). However, adjectives with '-' in it such as 'fifty-five' are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.pos_tag(tks)\n",
    "\n",
    "filtered_examples = []\n",
    "for example in examples:\n",
    "    match = example[1]\n",
    "    # for matches with 2 words\n",
    "    # examples: same percent, a percent\n",
    "    if len(match.split()) == 2:\n",
    "        # remove matches that begin with any adjectives (small, big, fixed, same) that is not a cardinal number 'CD'\n",
    "        # remove matches that begin with a verb\n",
    "        pos_of_first_word = nltk.pos_tag(match)[0][1]\n",
    "        # remove matches that begin with a stop word\n",
    "        if (match.split()[0].lower() not in stop_words) and (pos_of_first_word == 'CD'):\n",
    "            filtered_examples.append(example)\n",
    "        # fifty-five is an adjective ('JJ') but is kept since it contains a hyphen ('-')\n",
    "        elif (match.split()[0].lower() not in stop_words) and (pos_of_first_word == 'JJ') and ('-' in match.split()[0]):\n",
    "            filtered_examples.append(example)\n",
    "            # if false, don't do anything\n",
    "    # for matches with 3 words\n",
    "    # example: get to 2%\n",
    "    elif len(match.split()) == 3:\n",
    "        pos_of_first_word = nltk.pos_tag(match)[0][1]\n",
    "        if (match.split()[0].lower() not in stop_words) and (pos_of_first_word == 'CD'):\n",
    "            filtered_examples.append(example)\n",
    "        elif (match.split()[0].lower() not in stop_words) and (pos_of_first_word == 'JJ') and ('-' in match.split()[0]):\n",
    "            filtered_examples.append(example)\n",
    "        else:\n",
    "            # i.e. only get '2%' in 'get to 2%'\n",
    "            filtered_examples.append((example[0],example[1].split()[-1]))\n",
    "    else:\n",
    "        filtered_examples.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing percentages to a txt file\n",
    "percentage_file = 'all_percentages.txt'\n",
    "with open(percentage_file, 'a+') as the_file:\n",
    "    for example in filtered_examples:\n",
    "        the_file.write('(\"' + example[0] + '\", \"' + example[1] + '\")\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capitalized words regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [],
   "source": [
    "capitalized_regex = r'([A-Z][A-Za-z\\.]+(?=\\s[A-Z])(?:\\s[A-Z][A-Za-z\\.]+)+)'\n",
    "capitalized_examples = []\n",
    "for article in sentences:\n",
    "    for sentence in article:\n",
    "        matches = re.findall(capitalized_regex,sentence)\n",
    "        # if there are matches, then append (sentence, match) to list of examples\n",
    "        if len(matches):\n",
    "            for m in matches:\n",
    "                result = (sentence,m)\n",
    "                capitalized_examples.append(result)\n",
    "\n",
    "# remove any words that do not have a proper noun\n",
    "filtered_capitalized_examples = []\n",
    "for example in capitalized_examples:\n",
    "    has_NNP = False\n",
    "    for tag in nltk.pos_tag(word_tokenize(example[1])):\n",
    "        if tag[1] == 'NNP':\n",
    "            has_NNP = True\n",
    "    if has_NNP:\n",
    "        filtered_capitalized_examples.append(example)\n",
    "\n",
    "single_capitalized = r'([A-Z][A-Za-z\\.]+)(?:(?:\\s[a-z]+)|\\.)'\n",
    "single_cap_examples = []\n",
    "for article in sentences:\n",
    "    for sentence in article:\n",
    "        matches = re.findall(single_capitalized,sentence)\n",
    "        # if there are matches, then append (sentence, match) to list of examples\n",
    "        if len(matches):\n",
    "            for m in matches:\n",
    "                result = (sentence,m)\n",
    "                single_cap_examples.append(result)\n",
    "\n",
    "filtered_single_cap_examples = []\n",
    "for example in single_cap_examples:\n",
    "    tag = nltk.pos_tag([example[1]])[0][1]\n",
    "    # if the single word is not a stop word and is not a pronoun\n",
    "    if (example[1].lower() not in stop_words) and ('PRP' not in tag):\n",
    "        filtered_single_cap_examples.append(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting CEO names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data exploration to look for features of CEO names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop after 100000 samples, do not need every example\n",
    "ceo_examples = []\n",
    "\n",
    "# loop through all rows\n",
    "for i in range(ceo.shape[0]):\n",
    "    # create regex\n",
    "    first_name = ceo[0][i]\n",
    "    last_exists = (type(ceo[1][i]) == str)\n",
    "    first_exists = (type(ceo[0][i]) == str)\n",
    "    if last_exists and first_exists:\n",
    "        last_name = ceo[1][i]\n",
    "        # concatenate to create full name (first last)\n",
    "        full_name = first_name + ' ' + last_name\n",
    "        regex1 = re.escape(full_name)\n",
    "        # concatenante to create full name ver. 2 (last, first)\n",
    "        last_first = last_name + ', ' + first_name\n",
    "        regex2 = re.escape(last_first)\n",
    "    elif first_exists:\n",
    "        regex3 = re.escape(first_name)\n",
    "    \n",
    "    for article in sentences:\n",
    "        for sentence in article:\n",
    "            if last_exists and first_exists:\n",
    "                matches = re.findall(regex1,sentence)\n",
    "                if len(matches):\n",
    "                    for m in matches:\n",
    "                        result = (sentence,m)\n",
    "                        ceo_examples.append(result)\n",
    "                matches2 = re.findall(regex2,sentence)\n",
    "                if len(matches2):\n",
    "                    for m in matches2:\n",
    "                        result = (sentence,m)\n",
    "                        ceo_examples.append(result)\n",
    "            elif first_exists:\n",
    "                matches3 = re.findall(regex3,sentence)\n",
    "                if len(matches3):\n",
    "                    for m in matches3:\n",
    "                        result = (sentence,m)\n",
    "                        ceo_examples.append(result)\n",
    "    if len(ceo_examples) > 100000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Thinkorswim\\r\\nReutersAs a manager, JP Morgan CEO Jamie Dimon doesn't like when colleagues throw each other under the bus, MarketWatch's Sital Patel reports.\",\n",
       " 'Jamie Dimon')"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo_examples[100] # verb follows ceo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Like many California residents, Elon Musk hates driving on Interstate 405 at rush hour.',\n",
       " 'Elon Musk')"
      ]
     },
     "execution_count": 636,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo_examples[1011] # verb follows ceo name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"Incidentally, I don't remember hearing anything from Jamie Dimon at the time Chase was acquiring these banks about any reluctance to buy up two firms that had just spent years helping to blow up the world economic system with phony loans.\",\n",
       " 'Jamie Dimon')"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo_examples[106] # another proper noun (NNP) \"Chase\" is in sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating positive examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 877,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "pos_samples = random.sample(ceo_examples,255)\n",
    "\n",
    "true_pos_samples = []\n",
    "for sample in pos_samples:\n",
    "    if len(sample[1].split()) > 1:\n",
    "        true_pos_samples.append(sample)\n",
    "\n",
    "len(true_pos_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: NNP proper noun, number of proper noun tags in sentence that are not in match,contains 'CEO' in sentence, is a stop word in the match?, length of match, number of words in match,\"company\" in sentence verb follows it?, number of verb tags in sentence, contains a cardinal digit tag in the sentence\n",
    "colnames = ['is_NNP','num_NNP','contains_CEO','is_stop','match_len','num_words','followed_by_verb','num_verbs','contains_digit','contains_company','result']\n",
    "ceo_training_data = pd.DataFrame(columns=colnames)\n",
    "\n",
    "# positive examples\n",
    "for example in true_pos_samples:\n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    is_NNP = 1\n",
    "    for tag in match_tags:\n",
    "        is_NNP = is_NNP and (tag[1] == 'NNP')\n",
    "    if is_NNP:\n",
    "        is_NNP = 1\n",
    "    else:\n",
    "        is_NNP = 0\n",
    "    \n",
    "    num_NNP = 0\n",
    "    subtract_this = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            subtract_this += 1\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP += 1\n",
    "    num_NNP = num_NNP - subtract_this\n",
    "    \n",
    "    \n",
    "    contains_CEO = 'CEO' in sentence\n",
    "    if contains_CEO:\n",
    "        contains_CEO = 1\n",
    "    else:\n",
    "        contains_CEO = 0\n",
    "    \n",
    "    is_stop = 0\n",
    "    for word in match.split():\n",
    "        if word.lower() in stop_words:\n",
    "            is_stop = 1\n",
    "            break\n",
    "    \n",
    "    match_len = 0\n",
    "    for word in match.split():\n",
    "        match_len += len(word)\n",
    "    \n",
    "    num_words = len(match.split())\n",
    "    \n",
    "    for word in word_tokenize(sentence):\n",
    "        if word_tokenize(match)[-1] in word:\n",
    "            following_index = word_tokenize(sentence).index(word) + 1\n",
    "    if following_index >= len(sentence_tags):\n",
    "        followed_by_verb = 0\n",
    "    else:\n",
    "        followed_by_verb = 'VB' in sentence_tags[following_index][1] \n",
    "    if followed_by_verb:\n",
    "        followed_by_verb = 1\n",
    "    else:\n",
    "        followed_by_verb = 0\n",
    "    \n",
    "    num_verbs = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'VB'):\n",
    "            num_verbs += 1\n",
    "    \n",
    "    contains_digit = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            contains_digit = 1\n",
    "            break\n",
    "            \n",
    "    contains_company = ('compan' in sentence) or ('Compan' in sentence)\n",
    "    if contains_company:\n",
    "        contains_company = 1\n",
    "    else:\n",
    "        contains_company = 0\n",
    "    \n",
    "    result = 1\n",
    "    \n",
    "    ceo_training_data = ceo_training_data.append({\n",
    "     'is_NNP': is_NNP,\n",
    "     'num_NNP': num_NNP,\n",
    "     'contains_CEO': contains_CEO,\n",
    "     'is_stop': is_stop,\n",
    "     'match_len': match_len,\n",
    "     'num_words': num_words,\n",
    "     'followed_by_verb':followed_by_verb,\n",
    "     'num_verbs': num_verbs,\n",
    "     'contains_digit': contains_digit, \n",
    "     'contains_company': contains_company,\n",
    "     'result': result\n",
    "      }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 868,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually extract negative examples from corpus and saved into txt file\n",
    "# 196 negative samples extracted\n",
    "text_neg = []\n",
    "path_neg = './ceo_negative_examples'\n",
    "for file in os.listdir(path_neg):\n",
    "    text_neg.append(open(os.path.join(path_neg,file),'rb').read().decode(\"utf-8\",errors=\"replace\"))\n",
    "\n",
    "# list of all sentence tokenized articles\n",
    "sentences_neg = []\n",
    "for txt in text_neg:\n",
    "    sentences_neg.append(sent_tokenize(txt))\n",
    "    \n",
    "capitalized_regex = r'([A-Z][A-Za-z\\.]+(?=\\s[A-Z])(?:\\s[A-Z][A-Za-z\\.]+)+)'\n",
    "true_negative_samples = []\n",
    "for article in sentences_neg:\n",
    "    for sentence in article:\n",
    "        matches = re.findall(capitalized_regex,sentence)\n",
    "        # if there are matches, then append (sentence, match) to list of examples\n",
    "        if len(matches):\n",
    "            for m in matches:\n",
    "                result = (sentence,m)\n",
    "                true_negative_samples.append(result)\n",
    "                \n",
    "len(true_negative_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 11)"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for example in true_negative_samples:\n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    is_NNP = 1\n",
    "    for tag in match_tags:\n",
    "        is_NNP = is_NNP and (tag[1] == 'NNP')\n",
    "    if is_NNP:\n",
    "        is_NNP = 1\n",
    "    else:\n",
    "        is_NNP = 0\n",
    "    \n",
    "    num_NNP = 0\n",
    "    subtract_this = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            subtract_this += 1\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP += 1\n",
    "    num_NNP = num_NNP - subtract_this\n",
    "    \n",
    "    \n",
    "    contains_CEO = 'CEO' in sentence\n",
    "    if contains_CEO:\n",
    "        contains_CEO = 1\n",
    "    else:\n",
    "        contains_CEO = 0\n",
    "    \n",
    "    is_stop = 0\n",
    "    for word in match.split():\n",
    "        if word.lower() in stop_words:\n",
    "            is_stop = 1\n",
    "            break\n",
    "    \n",
    "    match_len = 0\n",
    "    for word in match.split():\n",
    "        match_len += len(word)\n",
    "    \n",
    "    num_words = len(match.split())\n",
    "    \n",
    "    for word in word_tokenize(sentence):\n",
    "        if word_tokenize(match)[-1] in word:\n",
    "            following_index = word_tokenize(sentence).index(word) + 1\n",
    "    if following_index >= len(sentence_tags):\n",
    "        followed_by_verb = 0\n",
    "    else:\n",
    "        followed_by_verb = 'VB' in sentence_tags[following_index][1] \n",
    "    if followed_by_verb:\n",
    "        followed_by_verb = 1\n",
    "    else:\n",
    "        followed_by_verb = 0\n",
    "    \n",
    "    num_verbs = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'VB'):\n",
    "            num_verbs += 1\n",
    "    \n",
    "    contains_digit = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            contains_digit = 1\n",
    "            break\n",
    "            \n",
    "    contains_company = ('compan' in sentence) or ('Compan' in sentence)\n",
    "    if contains_company:\n",
    "        contains_company = 1\n",
    "    else:\n",
    "        contains_company = 0\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    ceo_training_data = ceo_training_data.append({\n",
    "     'is_NNP': is_NNP,\n",
    "     'num_NNP': num_NNP,\n",
    "     'contains_CEO': contains_CEO,\n",
    "     'is_stop': is_stop,\n",
    "     'match_len': match_len,\n",
    "     'num_words': num_words,\n",
    "     'followed_by_verb':followed_by_verb,\n",
    "     'num_verbs': num_verbs,\n",
    "     'contains_digit': contains_digit, \n",
    "     'contains_company': contains_company,\n",
    "     'result': result\n",
    "      }, ignore_index=True)\n",
    "    \n",
    "ceo_training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification with logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceo_training_data = ceo_training_data.apply(pd.to_numeric)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = ceo_training_data.iloc[:,:-1]\n",
    "y = ceo_training_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24 23]\n",
      " [ 3 48]]\n",
      "Accuracy of logistic regression classifier on test set: 0.73\n",
      "Precision of logistic regression classifier on test set: 0.68\n",
      "Recall of logistic regression classifier on test set: 0.94\n"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))\n",
    "print('Precision of logistic regression classifier on test set: {:.2f}'.format(48/(48+23)))\n",
    "print('Recall of logistic regression classifier on test set: {:.2f}'.format(48/(48+3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifying the capitalized words as ceo names or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: NNP proper noun, number of proper noun tags in sentence that are not in match,contains 'CEO' in sentence, is a stop word in the match?, length of match, number of words in match, verb follows it?, number of verb tags in sentence\n",
    "colnames = ['is_NNP','num_NNP','contains_CEO','is_stop','match_len','num_words','followed_by_verb','num_verbs','contains_digit','contains_company']\n",
    "classified_ceo_names = []\n",
    "\n",
    "for example in filtered_capitalized_examples:\n",
    "    # initialize feature vector of a single example\n",
    "    feature_vector = pd.DataFrame(columns=colnames)\n",
    "    \n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    is_NNP = 1\n",
    "    for tag in match_tags:\n",
    "        is_NNP = is_NNP and (tag[1] == 'NNP')\n",
    "    if is_NNP:\n",
    "        is_NNP = 1\n",
    "    else:\n",
    "        is_NNP = 0\n",
    "    \n",
    "    num_NNP = 0\n",
    "    subtract_this = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            subtract_this += 1\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP += 1\n",
    "    num_NNP = num_NNP - subtract_this\n",
    "    \n",
    "    \n",
    "    contains_CEO = 'CEO' in sentence\n",
    "    if contains_CEO:\n",
    "        contains_CEO = 1\n",
    "    else:\n",
    "        contains_CEO = 0\n",
    "    \n",
    "    is_stop = 0\n",
    "    for word in match.split():\n",
    "        if word.lower() in stop_words:\n",
    "            is_stop = 1\n",
    "            break\n",
    "    \n",
    "    match_len = 0\n",
    "    for word in match.split():\n",
    "        match_len += len(word)\n",
    "    \n",
    "    num_words = len(match.split())\n",
    "    \n",
    "    for word in word_tokenize(sentence):\n",
    "        if word_tokenize(match)[-1] in word:\n",
    "            following_index = word_tokenize(sentence).index(word) + 1\n",
    "    if following_index >= len(sentence_tags):\n",
    "        followed_by_verb = 0\n",
    "    else:\n",
    "        followed_by_verb = 'VB' in sentence_tags[following_index][1] \n",
    "    if followed_by_verb:\n",
    "        followed_by_verb = 1\n",
    "    else:\n",
    "        followed_by_verb = 0\n",
    "    \n",
    "    num_verbs = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'VB'):\n",
    "            num_verbs += 1\n",
    "            \n",
    "    contains_digit = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            contains_digit = 1\n",
    "            break\n",
    "    \n",
    "    contains_company = ('compan' in sentence) or ('Compan' in sentence)\n",
    "    if contains_company:\n",
    "        contains_company = 1\n",
    "    else:\n",
    "        contains_company = 0\n",
    "    \n",
    "    feature_vector = feature_vector.append({\n",
    "     'is_NNP': is_NNP,\n",
    "     'num_NNP': num_NNP,\n",
    "     'contains_CEO': contains_CEO,\n",
    "     'is_stop': is_stop,\n",
    "     'match_len': match_len,\n",
    "     'num_words': num_words,\n",
    "     'followed_by_verb':followed_by_verb,\n",
    "     'contains_digit': contains_digit, \n",
    "     'contains_company': contains_company,\n",
    "     'num_verbs': num_verbs\n",
    "      }, ignore_index=True)\n",
    "    \n",
    "    predicted_result = classifier.predict(feature_vector)\n",
    "    if predicted_result:\n",
    "        classified_ceo_names.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing ceo names to a file\n",
    "ceo_file = 'all_ceo_names.txt'\n",
    "with open(ceo_file, 'a+') as the_file:\n",
    "    for example in classified_ceo_names:\n",
    "        the_file.write('(\"' + example[0] + '\", \"' + example[1] + '\")\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating positive examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 841,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop after 300,000 samples, do not need every example\n",
    "company_examples = []\n",
    "\n",
    "# loop through all rows\n",
    "for i in range(companies.shape[0]):\n",
    "    # create regex\n",
    "    company_name = companies[0][i]\n",
    "    co_regex = re.escape(company_name)\n",
    "    \n",
    "    for article in sentences:\n",
    "        for sentence in article:\n",
    "            matches = re.findall(co_regex,sentence)\n",
    "            if len(matches):\n",
    "                for m in matches:\n",
    "                    result = (sentence,m)\n",
    "                    company_examples.append(result)\n",
    "                    \n",
    "    if len(company_examples) > 300000:\n",
    "        break\n",
    "\n",
    "comp_pos_samples = random.sample(company_examples,151)\n",
    "len(comp_pos_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "#contains inc/co in sentence, contains inc/co in match, contains '$' in sentence, contains 'percent' or '%' in sentence, contains year (4 digit number regex) in sentence, number of CD's in sentence tags, comtains 'compan', number of proper nouns 'NNP' in match, contains 'stock' in sentence, contains 'quarter' in sentence, contains 'revenue' in sentence, contains 'share' in sentence, contains 'illion' (as in million/billion) in sentence\n",
    "colnames = ['inc_co_sent','inc_co_match','dollar_sign_sent','percent_sent','year_sent','num_CD','compan_sent','num_NNP_match','stock_sent','quarter_sent','revenue_sent', 'share_sent', 'illion_sent','result']\n",
    "comp_training_data = pd.DataFrame(columns=colnames)\n",
    "\n",
    "year_regex = r'\\d{4}'\n",
    "# positive examples\n",
    "for example in comp_pos_samples:\n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    if ('inc' in sentence.lower()) or ('co' in sentence.lower()):\n",
    "        inc_co_sent = 1\n",
    "    else:\n",
    "        inc_co_sent = 0\n",
    "       \n",
    "    if ('inc' in match.lower()) or ('co' in match.lower()):\n",
    "        inc_co_match = 1\n",
    "    else:\n",
    "        inc_co_match = 0\n",
    "    \n",
    "    if '$' in sentence:\n",
    "        dollar_sign_sent = 1\n",
    "    else:\n",
    "        dollar_sign_sent = 0\n",
    "    \n",
    "    if ('percent' in sentence.lower()) or ('%' in sentence.lower()):\n",
    "        percent_sent = 1\n",
    "    else:\n",
    "        percent_sent = 0\n",
    "    \n",
    "    if len(re.findall(year_regex,sentence)):\n",
    "        year_sent = 1\n",
    "    else:\n",
    "        year_sent = 0\n",
    "    \n",
    "    num_CD = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            num_verbs += 1\n",
    "    \n",
    "    if ('compan' in sentence.lower()):\n",
    "        compan_sent = 1\n",
    "    else:\n",
    "        compan_sent = 0\n",
    "        \n",
    "    num_NNP_match = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP_match += 1\n",
    "\n",
    "    if ('stock' in sentence.lower()):\n",
    "        stock_sent = 1\n",
    "    else:\n",
    "        stock_sent = 0\n",
    "    \n",
    "    if ('quarter' in sentence.lower()):\n",
    "        quarter_sent = 1\n",
    "    else:\n",
    "        quarter_sent = 0\n",
    "    \n",
    "    if ('revenue' in sentence.lower()):\n",
    "        revenue_sent = 1\n",
    "    else:\n",
    "        revenue_sent = 0\n",
    "    \n",
    "    if ('share' in sentence.lower()):\n",
    "        share_sent = 1\n",
    "    else:\n",
    "        share_sent = 0\n",
    "    \n",
    "    if ('illion' in sentence.lower()):\n",
    "        illion_sent = 1\n",
    "    else:\n",
    "        illion_sent = 0\n",
    "    \n",
    "    result = 1\n",
    "    \n",
    "    comp_training_data = comp_training_data.append({\n",
    "     'inc_co_sent': inc_co_sent,\n",
    "     'inc_co_match': inc_co_match,\n",
    "     'dollar_sign_sent': dollar_sign_sent,\n",
    "     'percent_sent': percent_sent,\n",
    "     'year_sent': year_sent,\n",
    "     'num_CD': num_CD,\n",
    "     'compan_sent': compan_sent,\n",
    "     'num_NNP_match': num_NNP_match,\n",
    "     'stock_sent': stock_sent,\n",
    "     'quarter_sent': quarter_sent,\n",
    "     'revenue_sent': revenue_sent,\n",
    "     'share_sent': share_sent,\n",
    "     'illion_sent': illion_sent,\n",
    "     'result': result\n",
    "      }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating negative examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 840,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually pull negative examples from corpus and saved into txt file\n",
    "comp_text_neg = []\n",
    "path_neg = './company_negative_examples'\n",
    "for file in os.listdir(path_neg):\n",
    "    comp_text_neg.append(open(os.path.join(path_neg,file),'rb').read().decode(\"utf-8\",errors=\"replace\"))\n",
    "\n",
    "# list of all sentence tokenized articles\n",
    "comp_sentences_neg = []\n",
    "for txt in comp_text_neg:\n",
    "    comp_sentences_neg.append(sent_tokenize(txt))\n",
    "    \n",
    "capitalized_regex = r'([A-Z][A-Za-z\\.]+(?=\\s[A-Z])(?:\\s[A-Z][A-Za-z\\.]+)+)'\n",
    "single_capitalized = r'([A-Z][A-Za-z\\.]+)(?:(?:\\s[a-z]+)|\\.)'\n",
    "comp_neg_samples = []\n",
    "for article in comp_sentences_neg:\n",
    "    for sentence in article:\n",
    "        \n",
    "        matches = re.findall(capitalized_regex,sentence)\n",
    "        # if there are matches, then append (sentence, match) to list of examples\n",
    "        if len(matches):\n",
    "            for m in matches:\n",
    "                result = (sentence,m)\n",
    "                comp_neg_samples.append(result)\n",
    "        \n",
    "        matches = re.findall(single_capitalized,sentence)\n",
    "        # if there are matches, then append (sentence, match) to list of examples\n",
    "        if len(matches):\n",
    "            for m in matches:\n",
    "                result = (sentence,m)\n",
    "                single_cap_examples.append(result)\n",
    "\n",
    "len(comp_neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(302, 14)"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative examples\n",
    "for example in comp_neg_samples:\n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    if ('inc' in sentence.lower()) or ('co' in sentence.lower()):\n",
    "        inc_co_sent = 1\n",
    "    else:\n",
    "        inc_co_sent = 0\n",
    "       \n",
    "    if ('inc' in match.lower()) or ('co' in match.lower()):\n",
    "        inc_co_match = 1\n",
    "    else:\n",
    "        inc_co_match = 0\n",
    "    \n",
    "    if '$' in sentence:\n",
    "        dollar_sign_sent = 1\n",
    "    else:\n",
    "        dollar_sign_sent = 0\n",
    "    \n",
    "    if ('percent' in sentence.lower()) or ('%' in sentence.lower()):\n",
    "        percent_sent = 1\n",
    "    else:\n",
    "        percent_sent = 0\n",
    "    \n",
    "    if len(re.findall(year_regex,sentence)):\n",
    "        year_sent = 1\n",
    "    else:\n",
    "        year_sent = 0\n",
    "    \n",
    "    num_CD = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            num_verbs += 1\n",
    "    \n",
    "    if ('compan' in sentence.lower()):\n",
    "        compan_sent = 1\n",
    "    else:\n",
    "        compan_sent = 0\n",
    "        \n",
    "    num_NNP_match = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP_match += 1\n",
    "\n",
    "    if ('stock' in sentence.lower()):\n",
    "        stock_sent = 1\n",
    "    else:\n",
    "        stock_sent = 0\n",
    "    \n",
    "    if ('quarter' in sentence.lower()):\n",
    "        quarter_sent = 1\n",
    "    else:\n",
    "        quarter_sent = 0\n",
    "    \n",
    "    if ('revenue' in sentence.lower()):\n",
    "        revenue_sent = 1\n",
    "    else:\n",
    "        revenue_sent = 0\n",
    "    \n",
    "    if ('share' in sentence.lower()):\n",
    "        share_sent = 1\n",
    "    else:\n",
    "        share_sent = 0\n",
    "    \n",
    "    if ('illion' in sentence.lower()):\n",
    "        illion_sent = 1\n",
    "    else:\n",
    "        illion_sent = 0\n",
    "    \n",
    "    result = 0\n",
    "    \n",
    "    comp_training_data = comp_training_data.append({\n",
    "     'inc_co_sent': inc_co_sent,\n",
    "     'inc_co_match': inc_co_match,\n",
    "     'dollar_sign_sent': dollar_sign_sent,\n",
    "     'percent_sent': percent_sent,\n",
    "     'year_sent': year_sent,\n",
    "     'num_CD': num_CD,\n",
    "     'compan_sent': compan_sent,\n",
    "     'num_NNP_match': num_NNP_match,\n",
    "     'stock_sent': stock_sent,\n",
    "     'quarter_sent': quarter_sent,\n",
    "     'revenue_sent': revenue_sent,\n",
    "     'share_sent': share_sent,\n",
    "     'illion_sent': illion_sent,\n",
    "     'result': result\n",
    "      }, ignore_index=True)\n",
    "\n",
    "comp_training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification with logisitic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training data: 113 positive, 113 negative, 226 total\n",
    "comp_training_data = comp_training_data.apply(pd.to_numeric)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = comp_training_data.iloc[:,:-1]\n",
    "y = comp_training_data.iloc[:,-1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "classifier = LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33  5]\n",
      " [ 4 34]]\n",
      "Accuracy of logistic regression classifier on test set: 0.88\n",
      "Precision of logistic regression classifier on test set: 0.87\n",
      "Recall of logistic regression classifier on test set: 0.89\n"
     ]
    }
   ],
   "source": [
    "# test data: 38 positive, 38 negative, 76 total\n",
    "y_pred = classifier.predict(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(confusion_matrix)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(classifier.score(X_test, y_test)))\n",
    "print('Precision of logistic regression classifier on test set: {:.2f}'.format(34/39))\n",
    "print('Recall of logistic regression classifier on test set: {:.2f}'.format(34/38))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifiying capitalized words as company or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = ['inc_co_sent','inc_co_match','dollar_sign_sent','percent_sent','year_sent','num_CD','compan_sent','num_NNP_match','stock_sent','quarter_sent','revenue_sent', 'share_sent', 'illion_sent']\n",
    "classified_company_names = []\n",
    "\n",
    "# multi-word capitalized examples\n",
    "for example in filtered_capitalized_examples:\n",
    "    # initialize feature vector of a single example\n",
    "    feature_vector = pd.DataFrame(columns=colnames)\n",
    "\n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    if ('inc' in sentence.lower()) or ('co' in sentence.lower()):\n",
    "        inc_co_sent = 1\n",
    "    else:\n",
    "        inc_co_sent = 0\n",
    "       \n",
    "    if ('inc' in match.lower()) or ('co' in match.lower()):\n",
    "        inc_co_match = 1\n",
    "    else:\n",
    "        inc_co_match = 0\n",
    "    \n",
    "    if '$' in sentence:\n",
    "        dollar_sign_sent = 1\n",
    "    else:\n",
    "        dollar_sign_sent = 0\n",
    "    \n",
    "    if ('percent' in sentence.lower()) or ('%' in sentence.lower()):\n",
    "        percent_sent = 1\n",
    "    else:\n",
    "        percent_sent = 0\n",
    "    \n",
    "    if len(re.findall(year_regex,sentence)):\n",
    "        year_sent = 1\n",
    "    else:\n",
    "        year_sent = 0\n",
    "    \n",
    "    num_CD = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            num_verbs += 1\n",
    "    \n",
    "    if ('compan' in sentence.lower()):\n",
    "        compan_sent = 1\n",
    "    else:\n",
    "        compan_sent = 0\n",
    "        \n",
    "    num_NNP_match = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP_match += 1\n",
    "\n",
    "    if ('stock' in sentence.lower()):\n",
    "        stock_sent = 1\n",
    "    else:\n",
    "        stock_sent = 0\n",
    "    \n",
    "    if ('quarter' in sentence.lower()):\n",
    "        quarter_sent = 1\n",
    "    else:\n",
    "        quarter_sent = 0\n",
    "    \n",
    "    if ('revenue' in sentence.lower()):\n",
    "        revenue_sent = 1\n",
    "    else:\n",
    "        revenue_sent = 0\n",
    "    \n",
    "    if ('share' in sentence.lower()):\n",
    "        share_sent = 1\n",
    "    else:\n",
    "        share_sent = 0\n",
    "    \n",
    "    if ('illion' in sentence.lower()):\n",
    "        illion_sent = 1\n",
    "    else:\n",
    "        illion_sent = 0\n",
    "    \n",
    "    feature_vector = feature_vector.append({\n",
    "     'inc_co_sent': inc_co_sent,\n",
    "     'inc_co_match': inc_co_match,\n",
    "     'dollar_sign_sent': dollar_sign_sent,\n",
    "     'percent_sent': percent_sent,\n",
    "     'year_sent': year_sent,\n",
    "     'num_CD': num_CD,\n",
    "     'compan_sent': compan_sent,\n",
    "     'num_NNP_match': num_NNP_match,\n",
    "     'stock_sent': stock_sent,\n",
    "     'quarter_sent': quarter_sent,\n",
    "     'revenue_sent': revenue_sent,\n",
    "     'share_sent': share_sent,\n",
    "     'illion_sent': illion_sent\n",
    "      }, ignore_index=True)\n",
    "    \n",
    "    predicted_result = classifier.predict(feature_vector)\n",
    "    if predicted_result:\n",
    "        classified_company_names.append(example)\n",
    "\n",
    "# single word capitalized examples\n",
    "for example in filtered_single_cap_examples:\n",
    "    # initialize feature vector of a single example\n",
    "    feature_vector = pd.DataFrame(columns=colnames)\n",
    "\n",
    "    sentence = example[0]\n",
    "    match = example[1]\n",
    "    match_tags = nltk.pos_tag(word_tokenize(match))\n",
    "    sentence_tags = nltk.pos_tag(word_tokenize(sentence))\n",
    "    \n",
    "    if ('inc' in sentence.lower()) or ('co' in sentence.lower()):\n",
    "        inc_co_sent = 1\n",
    "    else:\n",
    "        inc_co_sent = 0\n",
    "       \n",
    "    if ('inc' in match.lower()) or ('co' in match.lower()):\n",
    "        inc_co_match = 1\n",
    "    else:\n",
    "        inc_co_match = 0\n",
    "    \n",
    "    if '$' in sentence:\n",
    "        dollar_sign_sent = 1\n",
    "    else:\n",
    "        dollar_sign_sent = 0\n",
    "    \n",
    "    if ('percent' in sentence.lower()) or ('%' in sentence.lower()):\n",
    "        percent_sent = 1\n",
    "    else:\n",
    "        percent_sent = 0\n",
    "    \n",
    "    if len(re.findall(year_regex,sentence)):\n",
    "        year_sent = 1\n",
    "    else:\n",
    "        year_sent = 0\n",
    "    \n",
    "    num_CD = 0\n",
    "    for tag in sentence_tags:\n",
    "        if (tag[1] == 'CD'):\n",
    "            num_verbs += 1\n",
    "    \n",
    "    if ('compan' in sentence.lower()):\n",
    "        compan_sent = 1\n",
    "    else:\n",
    "        compan_sent = 0\n",
    "        \n",
    "    num_NNP_match = 0\n",
    "    for tag in match_tags:\n",
    "        if (tag[1] == 'NNP'):\n",
    "            num_NNP_match += 1\n",
    "\n",
    "    if ('stock' in sentence.lower()):\n",
    "        stock_sent = 1\n",
    "    else:\n",
    "        stock_sent = 0\n",
    "    \n",
    "    if ('quarter' in sentence.lower()):\n",
    "        quarter_sent = 1\n",
    "    else:\n",
    "        quarter_sent = 0\n",
    "    \n",
    "    if ('revenue' in sentence.lower()):\n",
    "        revenue_sent = 1\n",
    "    else:\n",
    "        revenue_sent = 0\n",
    "    \n",
    "    if ('share' in sentence.lower()):\n",
    "        share_sent = 1\n",
    "    else:\n",
    "        share_sent = 0\n",
    "    \n",
    "    if ('illion' in sentence.lower()):\n",
    "        illion_sent = 1\n",
    "    else:\n",
    "        illion_sent = 0\n",
    "    \n",
    "    feature_vector = feature_vector.append({\n",
    "     'inc_co_sent': inc_co_sent,\n",
    "     'inc_co_match': inc_co_match,\n",
    "     'dollar_sign_sent': dollar_sign_sent,\n",
    "     'percent_sent': percent_sent,\n",
    "     'year_sent': year_sent,\n",
    "     'num_CD': num_CD,\n",
    "     'compan_sent': compan_sent,\n",
    "     'num_NNP_match': num_NNP_match,\n",
    "     'stock_sent': stock_sent,\n",
    "     'quarter_sent': quarter_sent,\n",
    "     'revenue_sent': revenue_sent,\n",
    "     'share_sent': share_sent,\n",
    "     'illion_sent': illion_sent\n",
    "      }, ignore_index=True)\n",
    "    \n",
    "    predicted_result = classifier.predict(feature_vector)\n",
    "    if predicted_result:\n",
    "        classified_company_names.append(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing company names to a file\n",
    "company_file = 'all_company_names.txt'\n",
    "with open(company_file, 'a+') as the_file:\n",
    "    for example in classified_company_names:\n",
    "        the_file.write('(\"' + example[0] + '\", \"' + example[1] + '\")\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
